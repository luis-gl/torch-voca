{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('torch-voca': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "45f001dbb29f1ff8ac27f1467b8c19bf11779ac3ece6be3f7c65a11d2b6d32be"
   }
  },
  "interpreter": {
   "hash": "45f001dbb29f1ff8ac27f1467b8c19bf11779ac3ece6be3f7c65a11d2b6d32be"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Importando módulos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "source": [
    "## Carga de datos\n",
    "#### Depende de los scripts de run_training, config_parser, data_handler y batcher en la carpeta de utils"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_training import get_train_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checkpoint dir already exists ./training\n",
      "Use existing config ./training/config.pkl\n",
      "Loading data\n",
      "Loading face vertices\n",
      "Loading templates\n",
      "Loading raw audio\n",
      "Process audio\n",
      "Loading index maps\n",
      "Initialize data splits\n",
      "Initialize training, validation, and test indices\n",
      "sequence data missing FaceTalk_170811_03274_TA - sentence01\n",
      "sequence data missing FaceTalk_170811_03274_TA - sentence02\n",
      "sequence data missing FaceTalk_170811_03274_TA - sentence24\n",
      "sequence data missing FaceTalk_170913_03279_TA - sentence12\n",
      "sequence data missing FaceTalk_170913_03279_TA - sentence38\n",
      "sequence data missing FaceTalk_170912_03278_TA - sentence11\n",
      "sequence data missing FaceTalk_170809_00138_TA - sentence32\n"
     ]
    }
   ],
   "source": [
    "config, batcher = get_train_elements()"
   ]
  },
  {
   "source": [
    "## Conversión de datos a tensores\n",
    "#### Se convierte los tensores a tipo float32 (originalmente float64 o double para pytorch)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "### Data de entrenamiento"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processed audio:  torch.Size([128, 16, 29, 1]) torch.float32\nface vertices:  torch.Size([128, 5023, 3, 1]) torch.float32\nface templates:  torch.Size([128, 5023, 3, 1]) torch.float32\nsubject index:  torch.Size([128]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "processed_audio, face_vertices, face_templates, subject_idx = batcher.get_training_batch(config['batch_size'])\n",
    "\n",
    "processed_audio = np.expand_dims(processed_audio, -1)\n",
    "face_vertices = np.expand_dims(face_vertices, -1)\n",
    "face_templates = np.expand_dims(face_templates, -1)\n",
    "\n",
    "processed_audio = torch.from_numpy(processed_audio).type(torch.float32)\n",
    "face_vertices = torch.from_numpy(face_vertices).type(torch.float32)\n",
    "face_templates = torch.from_numpy(face_templates).type(torch.float32)\n",
    "subject_idx = torch.from_numpy(subject_idx)\n",
    "\n",
    "print(\"processed audio: \", processed_audio.shape, processed_audio.dtype)\n",
    "print(\"face vertices: \", face_vertices.shape, face_vertices.dtype)\n",
    "print(\"face templates: \", face_templates.shape, face_templates.dtype)\n",
    "print(\"subject index: \", subject_idx.shape, subject_idx.dtype)"
   ]
  },
  {
   "source": [
    "### Data de validación"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processed audio:  torch.Size([1024, 16, 29, 1]) torch.float32\nface vertices:  torch.Size([1024, 5023, 3, 1]) torch.float32\nface templates:  torch.Size([1024, 5023, 3, 1]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "num_training_subjects = batcher.get_num_training_subjects()\n",
    "val_processed_audio, val_face_vertices, val_face_templates, _ = batcher.get_validation_batch(config['batch_size'])\n",
    "\n",
    "val_processed_audio = np.expand_dims(np.tile(val_processed_audio, (num_training_subjects, 1, 1)), -1)\n",
    "val_face_vertices = np.expand_dims(np.tile(val_face_vertices, (num_training_subjects, 1, 1)), -1)\n",
    "val_face_templates = np.expand_dims(np.tile(val_face_templates, (num_training_subjects, 1, 1)), -1)\n",
    "\n",
    "val_processed_audio = torch.from_numpy(val_processed_audio).type(torch.float32)\n",
    "val_face_vertices = torch.from_numpy(val_face_vertices).type(torch.float32)\n",
    "val_face_templates = torch.from_numpy(val_face_templates).type(torch.float32)\n",
    "\n",
    "print(\"processed audio: \", val_processed_audio.shape, val_processed_audio.dtype)\n",
    "print(\"face vertices: \", val_face_vertices.shape, val_face_vertices.dtype)\n",
    "print(\"face templates: \", val_face_templates.shape, val_face_templates.dtype)"
   ]
  },
  {
   "source": [
    "#### Convertir las condiciones de sujetos a representaciones One Hot"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([128, 8])\n"
     ]
    }
   ],
   "source": [
    "condition = nn.functional.one_hot(subject_idx, batcher.get_num_training_subjects())\n",
    "print(condition.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1024, 8])\n"
     ]
    }
   ],
   "source": [
    "val_condition = np.reshape(np.repeat(np.arange(num_training_subjects)[:,np.newaxis],\n",
    "                repeats=config['num_consecutive_frames']*config['batch_size'], axis=-1), [-1,])\n",
    "val_condition = torch.from_numpy(val_condition)\n",
    "val_condition = nn.functional.one_hot(val_condition, batcher.get_num_training_subjects())\n",
    "print(val_condition.shape)"
   ]
  },
  {
   "source": [
    "## Speech Encoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Inicialización de pesos y bias\n",
    "#### Debido a que Pytorch no posee una implementación para inicializar los pesos con un muestreo truncado de la distribución normal, se ha utilizado la implementación extraída del foro en este enlace: https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/15"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.init_trunc_norm import truncated_normal_"
   ]
  },
  {
   "source": [
    "#### Se creó una clase personalizada para las capas convolucionales y fully connected para implementar la inicialización de pesos por capa."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, in_units, out_units, init_weights=None, weightini=0.1, bias=0.0):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(in_units, out_units)\n",
    "\n",
    "        # inicialización de pesos\n",
    "        if init_weights is not None:\n",
    "            self.layer.weight.data = init_weights\n",
    "        elif weightini == 0.0:\n",
    "            nn.init.constant_(self.layer.weight, weightini)\n",
    "        else:\n",
    "            #nn.init.normal_(self.layer.weight, std=weightini)\n",
    "            self.layer.weight.data = truncated_normal_(self.layer.weight.data, std=weightini)\n",
    "        \n",
    "        # inicialización de bias\n",
    "        nn.init.constant_(self.layer.bias, bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConv2d(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k_size, stride=(0,0), padding=(0,0), std_dev=0.02):\n",
    "        super().__init__()\n",
    "        self.conv_layer = nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=k_size, stride=stride, padding=padding)\n",
    "\n",
    "        # inicialización de pesos y bias\n",
    "        #nn.init.normal_(self.conv_layer.weight, std=std_dev)\n",
    "        self.conv_layer.weight.data = truncated_normal_(self.conv_layer.weight.data, std=std_dev)\n",
    "        #nn.init.normal_(self.conv_layer.bias, std=std_dev)\n",
    "        self.conv_layer.bias.data = truncated_normal_(self.conv_layer.bias.data, std=std_dev)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_layer(x)"
   ]
  },
  {
   "source": [
    "#### En este bloque de código se utiliza una capa de Batch Normalization. En la versión en Tensorflow, los autores introducen el tensor de dimensiones $[N,16,29,1]$, siendo que Tensorflow espera uno de la forma $[N,H,W,C]$. Por esto se crea la capa respectiva con \"num_features = 1\".\n",
    "#### Sin embargo, se probó que no hay diferencia entre utilizar el tensor de dimensiones $[N,16,1,29]$ con \"num_features = 29\", teniendo que intercambiar las dimensiones respectivas.\n",
    "#### También es necesario mencionar que para migrar la capa de Batch Normalization de Tensorflow a Pytorch, es necesario calcular el momentum (Pytorch) a partir del decay (Tensorflow), siendo la fórmula $momentum = 1 - decay$ según el foro: https://discuss.pytorch.org/t/convering-a-batch-normalization-layer-from-tf-to-pytorch/20407\n",
    "#### Se utiliza padding de $(1,0)$ ya que a diferencia de Tensorflow donde el padding se calcula si se utiliza \"SAME\", en Pytorch se debe especificar."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_encoding_dim = config['expression_dim']\n",
    "condition_speech_features = config['condition_speech_features']\n",
    "speech_encoder_size_factor = config['speech_encoder_size_factor']\n",
    "\n",
    "batch_norm = nn.BatchNorm2d(num_features=29, eps=1e-5, momentum=0.1)\n",
    "\n",
    "time_convs = nn.Sequential(\n",
    "            CustomConv2d(in_ch=37, out_ch=32, k_size=(3,1), stride=(2,1), padding=(1,0)),\n",
    "            nn.ReLU(), # [128, 32, 8, 1]\n",
    "            CustomConv2d(in_ch=32, out_ch=32, k_size=(3,1), stride=(2,1), padding=(1,0)),\n",
    "            nn.ReLU(), # [128, 32, 4, 1]\n",
    "            CustomConv2d(in_ch=32, out_ch=64, k_size=(3,1), stride=(2,1), padding=(1,0)),\n",
    "            nn.ReLU(), # [128, 64, 2, 1]\n",
    "            CustomConv2d(in_ch=64, out_ch=64, k_size=(3,1), stride=(2,1), padding=(1,0)),\n",
    "            nn.ReLU() # [128, 64, 1, 1]\n",
    "        )\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "\n",
    "fc_layers = nn.Sequential(\n",
    "            FCLayer(72, 128),\n",
    "            nn.Tanh(),\n",
    "            FCLayer(128, speech_encoding_dim)\n",
    "        )"
   ]
  },
  {
   "source": [
    "#### Debido a que Batch Normalization en Pytorch espera un tensor de la forma $[N, C, H, W]$ es necesario cambiar las dimensiones del tensor original de $[N, H, W, C]$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Si se usa num_features = 1 en la capa de BatchNorm, utilizar:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processed audio:  torch.Size([128, 1, 16, 29])\n"
     ]
    }
   ],
   "source": [
    "processed_audio = processed_audio.permute(0,3,1,2)\n",
    "print(\"processed audio: \", processed_audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processed audio:  torch.Size([1024, 1, 16, 29])\n"
     ]
    }
   ],
   "source": [
    "val_processed_audio = val_processed_audio.permute(0,3,1,2)\n",
    "print(\"processed audio: \", val_processed_audio.shape)"
   ]
  },
  {
   "source": [
    "#### Si se usa num_features = 29 en la capa de BatchNorm, utilizar:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processed audio:  torch.Size([128, 29, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "processed_audio = processed_audio.permute(0,2,1,3)\n",
    "print(\"processed audio: \", processed_audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processed audio:  torch.Size([1024, 29, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "val_processed_audio = val_processed_audio.permute(0,2,1,3)\n",
    "print(\"processed audio: \", val_processed_audio.shape)"
   ]
  },
  {
   "source": [
    "### Procesamiento en la Capa de Batch Normalization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Si se usa num_features = 1 en la capa de BatchNorm, utilizar el sgte. bloque de código:\n",
    "#### Debido a que las transformaciones intermedias se hacen a partir del código en Tensorflow, es necesario volver a transformar el tensor a la forma $[N,C,H,W]$ al final del proceso para poder introducir el dato a las capas convolucionales."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "features norm:  torch.Size([128, 1, 16, 29])\nfeatures:  torch.Size([128, 16, 29, 1])\nfeatures reshaped:  torch.Size([128, 16, 1, 29])\nfeature condition:  torch.Size([128, 1, 1, 8])\nfeature condition:  torch.Size([128, 16, 1, 8])\nfeatures reshaped:  torch.Size([128, 16, 1, 37])\nfeatures reshaped:  torch.Size([128, 37, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "features_norm = batch_norm(processed_audio)\n",
    "print(\"features norm: \", features_norm.shape)\n",
    "\n",
    "# Regresar a la forma [N,H,W,C] (tensorflow)  el dato\n",
    "features_norm = features_norm.permute(0, 2, 3, 1)\n",
    "print(\"features: \", features_norm.shape)\n",
    "\n",
    "speech_features_reshaped = torch.reshape(features_norm, (-1, features_norm.shape[1], 1, features_norm.shape[2]))\n",
    "print(\"features reshaped: \", speech_features_reshaped.shape)\n",
    "\n",
    "# función equivalente en pytorch a tf.transpose en tensores de n-dimensiones\n",
    "speech_feature_condition = torch.reshape(condition, (-1, condition.shape[1], 1, 1)).permute(0, 2, 3, 1)\n",
    "print(\"feature condition: \", speech_feature_condition.shape)\n",
    "\n",
    "speech_feature_condition = torch.tile(speech_feature_condition, (1, features_norm.shape[1], 1, 1))\n",
    "print(\"feature condition: \", speech_feature_condition.shape)\n",
    "\n",
    "# Condicionamiento entre los sujetos de entrenamiento\n",
    "speech_features_reshaped = torch.cat((speech_features_reshaped, speech_feature_condition), dim=-1)\n",
    "print(\"features reshaped: \", speech_features_reshaped.shape)\n",
    "\n",
    "# transformar el tensor a la forma de pytorch [N, C, H, W]\n",
    "speech_features_reshaped = speech_features_reshaped.permute(0, 3, 1, 2)\n",
    "print(\"features reshaped: \", speech_features_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "features norm:  torch.Size([1024, 1, 16, 29])\nfeatures:  torch.Size([1024, 16, 29, 1])\nfeatures reshaped:  torch.Size([1024, 16, 1, 29])\nfeature condition:  torch.Size([1024, 1, 1, 8])\nfeature condition:  torch.Size([1024, 16, 1, 8])\nfeatures reshaped:  torch.Size([1024, 16, 1, 37])\nfeatures reshaped:  torch.Size([128, 37, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "val_features_norm = batch_norm(val_processed_audio)\n",
    "print(\"features norm: \", val_features_norm.shape)\n",
    "\n",
    "val_features_norm = val_features_norm.permute(0, 2, 3, 1)\n",
    "print(\"features: \", val_features_norm.shape)\n",
    "\n",
    "val_speech_features_reshaped = torch.reshape(val_features_norm, (-1, val_features_norm.shape[1], 1, val_features_norm.shape[2]))\n",
    "print(\"features reshaped: \", val_speech_features_reshaped.shape)\n",
    "\n",
    "# función equivalente en pytorch a tf.transpose en tensores de n-dimensiones\n",
    "val_speech_feature_condition = torch.reshape(val_condition, (-1, val_condition.shape[1], 1, 1)).permute(0, 2, 3, 1)\n",
    "print(\"feature condition: \", val_speech_feature_condition.shape)\n",
    "\n",
    "val_speech_feature_condition = torch.tile(val_speech_feature_condition, (1, val_features_norm.shape[1], 1, 1))\n",
    "print(\"feature condition: \", val_speech_feature_condition.shape)\n",
    "\n",
    "# Condicionamiento entre los sujetos de validación\n",
    "val_speech_features_reshaped = torch.cat((val_speech_features_reshaped, val_speech_feature_condition), dim=-1)\n",
    "print(\"features reshaped: \", val_speech_features_reshaped.shape)\n",
    "\n",
    "val_speech_features_reshaped = val_speech_features_reshaped.permute(0, 3, 1, 2)\n",
    "print(\"features reshaped: \", speech_features_reshaped.shape)"
   ]
  },
  {
   "source": [
    "#### Si se usa num_features = 29 en la capa de BatchNorm, utilizar el sgte. bloque de código:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "features norm:  torch.Size([128, 29, 16, 1])\nfeature condition:  torch.Size([128, 8, 1, 1])\nfeature condition:  torch.Size([128, 8, 16, 1])\nfeatures reshaped:  torch.Size([128, 37, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "features_norm = batch_norm(processed_audio)\n",
    "print(\"features norm: \", features_norm.shape)\n",
    "\n",
    "speech_feature_condition = torch.reshape(condition, (-1, condition.shape[1], 1, 1))\n",
    "print(\"feature condition: \", speech_feature_condition.shape)\n",
    "\n",
    "speech_feature_condition = torch.tile(speech_feature_condition, (1, 1, features_norm.shape[2], 1))\n",
    "print(\"feature condition: \", speech_feature_condition.shape)\n",
    "\n",
    "# Condicionamiento entre los sujetos de entrenamiento\n",
    "speech_features_reshaped = torch.cat((features_norm, speech_feature_condition), dim=1)\n",
    "print(\"features reshaped: \", speech_features_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "features norm:  torch.Size([1024, 29, 16, 1])\nfeature condition:  torch.Size([1024, 8, 1, 1])\nfeature condition:  torch.Size([128, 8, 16, 1])\nfeatures reshaped:  torch.Size([128, 37, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "val_features_norm = batch_norm(val_processed_audio)\n",
    "print(\"features norm: \", val_features_norm.shape)\n",
    "\n",
    "val_speech_feature_condition = torch.reshape(val_condition, (-1, val_condition.shape[1], 1, 1))\n",
    "print(\"feature condition: \", val_speech_feature_condition.shape)\n",
    "\n",
    "val_speech_feature_condition = torch.tile(val_speech_feature_condition, (1, 1, val_features_norm.shape[2], 1))\n",
    "print(\"feature condition: \", speech_feature_condition.shape)\n",
    "\n",
    "# Condicionamiento entre los sujetos de validación\n",
    "val_speech_features_reshaped = torch.cat((val_features_norm, val_speech_feature_condition), dim=1)\n",
    "print(\"features reshaped: \", speech_features_reshaped.shape)"
   ]
  },
  {
   "source": [
    "### Procesamiento en las Capas Convolucionales\n",
    "#### Dado que, independiente de la variable \"num_features\" utilizada en la capa de BatchNorm, la entrada que utiliza las capas convolucionales siguen la forma $[N,C,H,W]$, no es necesario realizar alguna transformación adicional para los siguientes pasos ya que en ambos valores de la variable, la entrada es $[N,37,16,1]$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "after convs:  torch.Size([128, 64, 1, 1])\nflatten:  torch.Size([128, 64])\n"
     ]
    }
   ],
   "source": [
    "features = time_convs(speech_features_reshaped)\n",
    "print(\"after convs: \", features.shape)\n",
    "features_flat = flatten(features)\n",
    "print(\"flatten: \", features_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "after convs:  torch.Size([1024, 64, 1, 1])\nflatten:  torch.Size([1024, 64])\n"
     ]
    }
   ],
   "source": [
    "val_features = time_convs(val_speech_features_reshaped)\n",
    "print(\"after convs: \", val_features.shape)\n",
    "val_features_flat = flatten(val_features)\n",
    "print(\"flatten: \", val_features_flat.shape)"
   ]
  },
  {
   "source": [
    "### Conversión de los Datos a un Espacio Latente"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "concat:  torch.Size([128, 72])\nfc result:  torch.Size([128, 50])\n"
     ]
    }
   ],
   "source": [
    "# Condicionamiento entre los sujetos de entrenamiento\n",
    "concatenated = torch.cat((features_flat, condition), dim=1)\n",
    "print(\"concat: \", concatenated.shape)\n",
    "fc_result = fc_layers(concatenated)\n",
    "print(\"fc result: \", fc_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "concat:  torch.Size([1024, 72])\nfc result:  torch.Size([1024, 50])\n"
     ]
    }
   ],
   "source": [
    "# Condicionamiento entre los sujetos de validación\n",
    "val_concatenated = torch.cat((val_features_flat, val_condition), dim=1)\n",
    "print(\"concat: \", val_concatenated.shape)\n",
    "val_fc_result = fc_layers(val_concatenated)\n",
    "print(\"fc result: \", val_fc_result.shape)"
   ]
  },
  {
   "source": [
    "## Speech Decoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_basis_fname = config['expression_basis_fname']\n",
    "init_expression = config['init_expression']\n",
    "\n",
    "num_vertices = config['num_vertices']\n",
    "expression_dim = config['expression_dim']"
   ]
  },
  {
   "source": [
    "#### Los autores inicializan los pesos a partir de un archivo que brindan en su carpeta de datos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([15069, 50])\n"
     ]
    }
   ],
   "source": [
    "init_exp_basis = np.zeros((3*num_vertices, expression_dim))\n",
    "\n",
    "if init_expression:\n",
    "    init_exp_basis[:, :min(expression_dim, 100)] = np.load(expression_basis_fname)[:, :min(expression_dim, 100)]\n",
    "\n",
    "init_exp_basis = torch.from_numpy(init_exp_basis).type(torch.float32)\n",
    "print(init_exp_basis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = FCLayer(in_units=expression_dim, out_units=3*num_vertices, init_weights=init_exp_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([128, 15069])\ntorch.Size([128, 5023, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "exp_offset = decoder(fc_result)\n",
    "print(exp_offset.shape)\n",
    "exp_offset = torch.reshape(exp_offset, (-1, num_vertices, 3, 1))\n",
    "print(exp_offset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1024, 15069])\ntorch.Size([1024, 5023, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "val_exp_offset = decoder(val_fc_result)\n",
    "print(val_exp_offset.shape)\n",
    "val_exp_offset = torch.reshape(val_exp_offset, (-1, num_vertices, 3, 1))\n",
    "print(val_exp_offset.shape)"
   ]
  },
  {
   "source": [
    "#### Como el resultado de la red es las distancias de traslaciones de los vértices y no los mismos vértices trasladados, es necesario sumarlo al modelo 3D base (Operación de Traslación en Modelos 3D)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([128, 5023, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "predicted = exp_offset + face_templates\n",
    "print(predicted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1024, 5023, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "val_predicted = val_exp_offset + val_face_templates\n",
    "print(val_predicted.shape)"
   ]
  }
 ]
}