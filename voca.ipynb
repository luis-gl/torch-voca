{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd045f001dbb29f1ff8ac27f1467b8c19bf11779ac3ece6be3f7c65a11d2b6d32be",
   "display_name": "Python 3.6.9 64-bit ('torch-voca': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "45f001dbb29f1ff8ac27f1467b8c19bf11779ac3ece6be3f7c65a11d2b6d32be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Importando módulos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "source": [
    "## Disponibilidad de dispositivo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Usando cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Usando {}'.format(device))"
   ]
  },
  {
   "source": [
    "## Carga de datos\n",
    "* Depende de los scripts de run_training, config_parser, data_handler y batcher"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_training import get_train_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checkpoint dir already exists ./training\n",
      "Use existing config ./training/config.pkl\n",
      "Loading data\n",
      "Loading face vertices\n",
      "Loading templates\n",
      "Loading raw audio\n",
      "Process audio\n",
      "Loading index maps\n",
      "Initialize data splits\n",
      "Initialize training, validation, and test indices\n",
      "sequence data missing FaceTalk_170811_03274_TA - sentence01\n",
      "sequence data missing FaceTalk_170811_03274_TA - sentence02\n",
      "sequence data missing FaceTalk_170811_03274_TA - sentence24\n",
      "sequence data missing FaceTalk_170913_03279_TA - sentence12\n",
      "sequence data missing FaceTalk_170913_03279_TA - sentence38\n",
      "sequence data missing FaceTalk_170912_03278_TA - sentence11\n",
      "sequence data missing FaceTalk_170809_00138_TA - sentence32\n"
     ]
    }
   ],
   "source": [
    "config, data_handler, batcher = get_train_elements()"
   ]
  },
  {
   "source": [
    "* Conversión de datos a tensores de tipo float32 (originalmente float64 o double para pytorch)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processed audio:  torch.Size([128, 16, 29]) torch.float32\nface vertices:  torch.Size([128, 5023, 3]) torch.float32\nface templates:  torch.Size([128, 5023, 3]) torch.float32\nsubject index:  torch.Size([128]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "processed_audio, face_vertices, face_templates, subject_idx = batcher.get_training_batch(config['batch_size'])\n",
    "processed_audio = torch.from_numpy(processed_audio).type(torch.float32)\n",
    "face_vertices = torch.from_numpy(face_vertices).type(torch.float32)\n",
    "face_templates = torch.from_numpy(face_templates).type(torch.float32)\n",
    "subject_idx = torch.from_numpy(subject_idx)\n",
    "print(\"processed audio: \", processed_audio.shape, processed_audio.dtype)\n",
    "print(\"face vertices: \", face_vertices.shape, face_vertices.dtype)\n",
    "print(\"face templates: \", face_templates.shape, face_templates.dtype)\n",
    "print(\"subject index: \", subject_idx.shape, subject_idx.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([128, 8])\n"
     ]
    }
   ],
   "source": [
    "condition = nn.functional.one_hot(subject_idx, batcher.get_num_training_subjects())\n",
    "print(condition.shape)"
   ]
  },
  {
   "source": [
    "## Speech Encoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_encoding_dim = config['expression_dim']\n",
    "condition_speech_features = config['condition_speech_features']\n",
    "speech_encoder_size_factor = config['speech_encoder_size_factor']\n",
    "\n",
    "batch_norm = nn.BatchNorm1d(num_features=29, eps=1e-5, momentum=0.9)\n",
    "\n",
    "time_convs = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=37, out_channels=32, kernel_size=(1,3), stride=(1,2), padding=(0,1)),\n",
    "            nn.ReLU(), # [128, 32, 1, 8]\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1,3), stride=(1,2), padding=(0,1)),\n",
    "            nn.ReLU(), # [128, 32, 1, 4]\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,3), stride=(1,2), padding=(0,1)),\n",
    "            nn.ReLU(), # [128, 64, 1, 2]\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(1,3), stride=(1,2), padding=(0,1)),\n",
    "            nn.ReLU(), # [128, 64, 1, 1]\n",
    "        )\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "\n",
    "fc_layers = nn.Sequential(\n",
    "            nn.Linear(72, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, speech_encoding_dim)\n",
    "        )"
   ]
  },
  {
   "source": [
    "* Debido a que el Batch Normalization espera un tensor de la forma $[128, 29, 16]$ es necesario cambiar las dimensiones del tensor original de $[128, 16, 29]$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processed audio:  torch.Size([128, 29, 16])\n"
     ]
    }
   ],
   "source": [
    "processed_audio = processed_audio.permute(0,2,1)\n",
    "print(\"processed audio: \", processed_audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "features:  torch.Size([128, 29, 16])\nfeatures reshaped:  torch.Size([128, 29, 1, 16])\ncondition reshaped:  torch.Size([128, 8, 1, 1])\nfeature condition:  torch.Size([128, 8, 1, 1])\nfeature condition:  torch.Size([128, 8, 1, 16])\nfeatures reshaped:  torch.Size([128, 37, 1, 16])\n"
     ]
    }
   ],
   "source": [
    "features_norm = batch_norm(processed_audio)\n",
    "print(\"features: \", features_norm.shape)\n",
    "speech_features_reshaped = torch.reshape(features_norm, (-1, features_norm.shape[1], 1, features_norm.shape[2]))\n",
    "print(\"features reshaped: \", speech_features_reshaped.shape)\n",
    "condition_reshaped = torch.reshape(condition, (-1, condition.shape[1], 1, 1))\n",
    "print(\"condition reshaped: \", condition_reshaped.shape)\n",
    "# función equivalente en pytorch a tf.transpose en tensores de n-dimensiones\n",
    "speech_feature_condition = condition_reshaped.permute(0, 1, 3, 2)\n",
    "print(\"feature condition: \", speech_feature_condition.shape)\n",
    "speech_feature_condition = torch.tile(speech_feature_condition, (1, 1, 1, features_norm.shape[2]))\n",
    "print(\"feature condition: \", speech_feature_condition.shape)\n",
    "speech_features_reshaped = torch.cat((speech_features_reshaped, speech_feature_condition), dim=1)\n",
    "print(\"features reshaped: \", speech_features_reshaped.shape)"
   ]
  },
  {
   "source": [
    "* Se utiliza padding a diferencia de la versión en Tensorflow debido a que los cálculos no resultarían como lo dicho en el paper:\n",
    "    * Primera capa sin padding: $[128, 32, 1, 7]$\n",
    "    * Segunda capa sin padding: $[128, 32, 1, 3]$\n",
    "    * Tercera capa sin padding: $[128, 64, 1, 1]$\n",
    "    * Cuarta capa sin padding: $indefinido$ (no se puede aplicar un kernel de $1\\times3$)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([128, 64, 1, 1])\ntorch.Size([128, 64])\n"
     ]
    }
   ],
   "source": [
    "features = time_convs(speech_features_reshaped)\n",
    "print(features.shape)\n",
    "features_flat = flatten(features)\n",
    "print(features_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([128, 72])\ntorch.Size([128, 50])\n"
     ]
    }
   ],
   "source": [
    "concatenated = torch.cat((features_flat, condition), dim=1)\n",
    "print(concatenated.shape)\n",
    "fc_result = fc_layers(concatenated)\n",
    "print(fc_result.shape)"
   ]
  },
  {
   "source": [
    "## Speech Decoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_basis_fname = config['expression_basis_fname']\n",
    "init_expression = config['init_expression']\n",
    "\n",
    "num_vertices = config['num_vertices']\n",
    "expression_dim = config['expression_dim']\n",
    "\n",
    "decoder = nn.Linear(expression_dim, 3*num_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([128, 15069])\ntorch.Size([128, 5023, 3])\n"
     ]
    }
   ],
   "source": [
    "exp_offset = decoder(fc_result)\n",
    "print(exp_offset.shape)\n",
    "exp_offset = torch.reshape(exp_offset, (-1, num_vertices, 3))\n",
    "print(exp_offset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([128, 5023, 3])\n"
     ]
    }
   ],
   "source": [
    "predicted = exp_offset + face_templates\n",
    "print(predicted.shape)"
   ]
  }
 ]
}