{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('torch-voca': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "45f001dbb29f1ff8ac27f1467b8c19bf11779ac3ece6be3f7c65a11d2b6d32be"
   }
  },
  "interpreter": {
   "hash": "45f001dbb29f1ff8ac27f1467b8c19bf11779ac3ece6be3f7c65a11d2b6d32be"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Importando módulos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "source": [
    "## Carga de datos\n",
    "* Depende de los scripts de run_training, config_parser, data_handler y batcher"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_training import get_train_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checkpoint dir already exists ./training\n",
      "Use existing config ./training/config.pkl\n",
      "Loading data\n",
      "Loading face vertices\n",
      "Loading templates\n",
      "Loading raw audio\n",
      "Process audio\n",
      "Loading index maps\n",
      "Initialize data splits\n",
      "Initialize training, validation, and test indices\n",
      "sequence data missing FaceTalk_170811_03274_TA - sentence01\n",
      "sequence data missing FaceTalk_170811_03274_TA - sentence02\n",
      "sequence data missing FaceTalk_170811_03274_TA - sentence24\n",
      "sequence data missing FaceTalk_170913_03279_TA - sentence12\n",
      "sequence data missing FaceTalk_170913_03279_TA - sentence38\n",
      "sequence data missing FaceTalk_170912_03278_TA - sentence11\n",
      "sequence data missing FaceTalk_170809_00138_TA - sentence32\n"
     ]
    }
   ],
   "source": [
    "config, batcher = get_train_elements()"
   ]
  },
  {
   "source": [
    "* Conversión de datos a tensores de tipo float32 (originalmente float64 o double para pytorch)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processed audio:  torch.Size([128, 16, 29, 1]) torch.float32\nface vertices:  torch.Size([128, 5023, 3, 1]) torch.float32\nface templates:  torch.Size([128, 5023, 3, 1]) torch.float32\nsubject index:  torch.Size([128]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "processed_audio, face_vertices, face_templates, subject_idx = batcher.get_training_batch(config['batch_size'])\n",
    "\n",
    "processed_audio = np.expand_dims(processed_audio, -1)\n",
    "face_vertices = np.expand_dims(face_vertices, -1)\n",
    "face_templates = np.expand_dims(face_templates, -1)\n",
    "\n",
    "processed_audio = torch.from_numpy(processed_audio).type(torch.float32)\n",
    "face_vertices = torch.from_numpy(face_vertices).type(torch.float32)\n",
    "face_templates = torch.from_numpy(face_templates).type(torch.float32)\n",
    "subject_idx = torch.from_numpy(subject_idx)\n",
    "\n",
    "print(\"processed audio: \", processed_audio.shape, processed_audio.dtype)\n",
    "print(\"face vertices: \", face_vertices.shape, face_vertices.dtype)\n",
    "print(\"face templates: \", face_templates.shape, face_templates.dtype)\n",
    "print(\"subject index: \", subject_idx.shape, subject_idx.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processed audio:  torch.Size([1024, 16, 29, 1]) torch.float32\nface vertices:  torch.Size([1024, 5023, 3, 1]) torch.float32\nface templates:  torch.Size([1024, 5023, 3, 1]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "num_training_subjects = batcher.get_num_training_subjects()\n",
    "val_processed_audio, val_face_vertices, val_face_templates, _ = batcher.get_validation_batch(config['batch_size'])\n",
    "\n",
    "val_processed_audio = np.expand_dims(np.tile(val_processed_audio, (num_training_subjects, 1, 1)), -1)\n",
    "val_face_vertices = np.expand_dims(np.tile(val_face_vertices, (num_training_subjects, 1, 1)), -1)\n",
    "val_face_templates = np.expand_dims(np.tile(val_face_templates, (num_training_subjects, 1, 1)), -1)\n",
    "\n",
    "val_processed_audio = torch.from_numpy(val_processed_audio).type(torch.float32)\n",
    "val_face_vertices = torch.from_numpy(val_face_vertices).type(torch.float32)\n",
    "val_face_templates = torch.from_numpy(val_face_templates).type(torch.float32)\n",
    "\n",
    "print(\"processed audio: \", val_processed_audio.shape, val_processed_audio.dtype)\n",
    "print(\"face vertices: \", val_face_vertices.shape, val_face_vertices.dtype)\n",
    "print(\"face templates: \", val_face_templates.shape, val_face_templates.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([128, 8])\n"
     ]
    }
   ],
   "source": [
    "condition = nn.functional.one_hot(subject_idx, batcher.get_num_training_subjects())\n",
    "print(condition.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1024, 8])\n"
     ]
    }
   ],
   "source": [
    "val_condition = np.reshape(np.repeat(np.arange(num_training_subjects)[:,np.newaxis],\n",
    "                repeats=config['num_consecutive_frames']*config['batch_size'], axis=-1), [-1,])\n",
    "val_condition = torch.from_numpy(val_condition)\n",
    "val_condition = nn.functional.one_hot(val_condition, batcher.get_num_training_subjects())\n",
    "print(val_condition.shape)"
   ]
  },
  {
   "source": [
    "## Speech Encoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, in_units, out_units, init_weights=None, weightini=0.1, bias=0.0):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(in_units, out_units)\n",
    "\n",
    "        # inicialización de pesos\n",
    "        if init_weights is not None:\n",
    "            self.layer.weight.data = init_weights\n",
    "        elif weightini == 0.0:\n",
    "            nn.init.constant_(self.layer.weight, weightini)\n",
    "        else:\n",
    "            nn.init.normal_(self.layer.weight, std=weightini)\n",
    "        \n",
    "        # inicialización de bias\n",
    "        nn.init.constant_(self.layer.bias, bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConv2d(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k_size, stride=(0,0), padding=(0,0), std_dev=0.02):\n",
    "        super().__init__()\n",
    "        self.conv_layer = nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=k_size, stride=stride, padding=padding)\n",
    "\n",
    "        # inicialización de pesos y bias\n",
    "        nn.init.normal_(self.conv_layer.weight, std=std_dev)\n",
    "        nn.init.normal_(self.conv_layer.bias, std=std_dev)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_encoding_dim = config['expression_dim']\n",
    "condition_speech_features = config['condition_speech_features']\n",
    "speech_encoder_size_factor = config['speech_encoder_size_factor']\n",
    "\n",
    "# En tensorflow, el batchnorm usa el formato [N, H, W, C] y los autores ingresan el tensor de [N, 16, 29, 1] tal cual\n",
    "# en tf, se usa decay en vez de momentum, momentum = 1 - decay segun el foro\n",
    "# https://discuss.pytorch.org/t/convering-a-batch-normalization-layer-from-tf-to-pytorch/20407\n",
    "batch_norm = nn.BatchNorm2d(num_features=1, eps=1e-5, momentum=0.1)\n",
    "\n",
    "time_convs = nn.Sequential(\n",
    "            CustomConv2d(in_ch=37, out_ch=32, k_size=(3,1), stride=(2,1), padding=(1,0)),\n",
    "            nn.ReLU(), # [128, 32, 8, 1]\n",
    "            CustomConv2d(in_ch=32, out_ch=32, k_size=(3,1), stride=(2,1), padding=(1,0)),\n",
    "            nn.ReLU(), # [128, 32, 4, 1]\n",
    "            CustomConv2d(in_ch=32, out_ch=64, k_size=(3,1), stride=(2,1), padding=(1,0)),\n",
    "            nn.ReLU(), # [128, 64, 2, 1]\n",
    "            CustomConv2d(in_ch=64, out_ch=64, k_size=(3,1), stride=(2,1), padding=(1,0)),\n",
    "            nn.ReLU() # [128, 64, 1, 1]\n",
    "        )\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "\n",
    "fc_layers = nn.Sequential(\n",
    "            FCLayer(72, 128),\n",
    "            nn.Tanh(),\n",
    "            FCLayer(128, speech_encoding_dim)\n",
    "        )"
   ]
  },
  {
   "source": [
    "* Debido a que el Batch Normalization espera un tensor de la forma $[N, C, H, W]$ es necesario cambiar las dimensiones del tensor original de $[N, H, W, C]$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processed audio:  torch.Size([128, 1, 16, 29])\n"
     ]
    }
   ],
   "source": [
    "processed_audio = processed_audio.permute(0,3,1,2)\n",
    "print(\"processed audio: \", processed_audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processed audio:  torch.Size([1024, 1, 16, 29])\n"
     ]
    }
   ],
   "source": [
    "val_processed_audio = val_processed_audio.permute(0,3,1,2)\n",
    "print(\"processed audio: \", val_processed_audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "features:  torch.Size([128, 1, 16, 29])\nfeatures:  torch.Size([128, 16, 29, 1])\nfeatures reshaped:  torch.Size([128, 16, 1, 29])\nfeature condition:  torch.Size([128, 1, 1, 8])\nfeature condition:  torch.Size([128, 16, 1, 8])\nfeatures reshaped:  torch.Size([128, 16, 1, 37])\nfeatures reshaped:  torch.Size([128, 37, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "features_norm = batch_norm(processed_audio)\n",
    "print(\"features: \", features_norm.shape)\n",
    "# Regresar a la forma  el dato\n",
    "features_norm = features_norm.permute(0, 2, 3, 1)\n",
    "print(\"features: \", features_norm.shape)\n",
    "\n",
    "speech_features_reshaped = torch.reshape(features_norm, (-1, features_norm.shape[1], 1, features_norm.shape[2]))\n",
    "print(\"features reshaped: \", speech_features_reshaped.shape)\n",
    "\n",
    "# función equivalente en pytorch a tf.transpose en tensores de n-dimensiones\n",
    "speech_feature_condition = torch.reshape(condition, (-1, condition.shape[1], 1, 1)).permute(0, 2, 3, 1)#(0, 1, 3, 2)\n",
    "print(\"feature condition: \", speech_feature_condition.shape)\n",
    "\n",
    "speech_feature_condition = torch.tile(speech_feature_condition, (1, features_norm.shape[1], 1, 1))\n",
    "print(\"feature condition: \", speech_feature_condition.shape)\n",
    "\n",
    "speech_features_reshaped = torch.cat((speech_features_reshaped, speech_feature_condition), dim=-1)\n",
    "print(\"features reshaped: \", speech_features_reshaped.shape)\n",
    "\n",
    "# transformar el tensor a la forma de pytorch [N, C, H, W]\n",
    "speech_features_reshaped = speech_features_reshaped.permute(0, 3, 1, 2)\n",
    "print(\"features reshaped: \", speech_features_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "features:  torch.Size([1024, 1, 16, 29])\nfeatures:  torch.Size([1024, 16, 29, 1])\nfeatures reshaped:  torch.Size([1024, 16, 1, 29])\nfeature condition:  torch.Size([1024, 1, 1, 8])\nfeature condition:  torch.Size([1024, 16, 1, 8])\nfeatures reshaped:  torch.Size([1024, 16, 1, 37])\nfeatures reshaped:  torch.Size([128, 37, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "val_features_norm = batch_norm(val_processed_audio)\n",
    "print(\"features: \", val_features_norm.shape)\n",
    "\n",
    "val_features_norm = val_features_norm.permute(0, 2, 3, 1)\n",
    "print(\"features: \", val_features_norm.shape)\n",
    "\n",
    "val_speech_features_reshaped = torch.reshape(val_features_norm, (-1, val_features_norm.shape[1], 1, val_features_norm.shape[2]))\n",
    "print(\"features reshaped: \", val_speech_features_reshaped.shape)\n",
    "\n",
    "# función equivalente en pytorch a tf.transpose en tensores de n-dimensiones\n",
    "val_speech_feature_condition = torch.reshape(val_condition, (-1, val_condition.shape[1], 1, 1)).permute(0, 2, 3, 1)\n",
    "print(\"feature condition: \", val_speech_feature_condition.shape)\n",
    "\n",
    "val_speech_feature_condition = torch.tile(val_speech_feature_condition, (1, val_features_norm.shape[1], 1, 1))\n",
    "print(\"feature condition: \", val_speech_feature_condition.shape)\n",
    "\n",
    "val_speech_features_reshaped = torch.cat((val_speech_features_reshaped, val_speech_feature_condition), dim=-1)\n",
    "print(\"features reshaped: \", val_speech_features_reshaped.shape)\n",
    "\n",
    "val_speech_features_reshaped = val_speech_features_reshaped.permute(0, 3, 1, 2)\n",
    "print(\"features reshaped: \", speech_features_reshaped.shape)"
   ]
  },
  {
   "source": [
    "* Se utiliza padding ya que a diferencia de Tensorflow donde el padding se calcula si se utiliza \"SAME\", en Pytorch se debe especificar."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "after convs:  torch.Size([128, 64, 1, 1])\nflatten:  torch.Size([128, 64])\n"
     ]
    }
   ],
   "source": [
    "features = time_convs(speech_features_reshaped)\n",
    "print(\"after convs: \", features.shape)\n",
    "features_flat = flatten(features)\n",
    "print(\"flatten: \", features_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "after convs:  torch.Size([1024, 64, 1, 1])\nflatten:  torch.Size([1024, 64])\n"
     ]
    }
   ],
   "source": [
    "val_features = time_convs(val_speech_features_reshaped)\n",
    "print(\"after convs: \", val_features.shape)\n",
    "val_features_flat = flatten(val_features)\n",
    "print(\"flatten: \", val_features_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "concat:  torch.Size([128, 72])\nfc result:  torch.Size([128, 50])\n"
     ]
    }
   ],
   "source": [
    "concatenated = torch.cat((features_flat, condition), dim=1)\n",
    "print(\"concat: \", concatenated.shape)\n",
    "fc_result = fc_layers(concatenated)\n",
    "print(\"fc result: \", fc_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "concat:  torch.Size([1024, 72])\nfc result:  torch.Size([1024, 50])\n"
     ]
    }
   ],
   "source": [
    "val_concatenated = torch.cat((val_features_flat, val_condition), dim=1)\n",
    "print(\"concat: \", val_concatenated.shape)\n",
    "val_fc_result = fc_layers(val_concatenated)\n",
    "print(\"fc result: \", val_fc_result.shape)"
   ]
  },
  {
   "source": [
    "## Speech Decoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_basis_fname = config['expression_basis_fname']\n",
    "init_expression = config['init_expression']\n",
    "\n",
    "num_vertices = config['num_vertices']\n",
    "expression_dim = config['expression_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([15069, 50])\n"
     ]
    }
   ],
   "source": [
    "init_exp_basis = np.zeros((3*num_vertices, expression_dim))\n",
    "\n",
    "if init_expression:\n",
    "    init_exp_basis[:, :min(expression_dim, 100)] = np.load(expression_basis_fname)[:, :min(expression_dim, 100)]\n",
    "\n",
    "init_exp_basis = torch.from_numpy(init_exp_basis).type(torch.float32)\n",
    "print(init_exp_basis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = FCLayer(in_units=expression_dim, out_units=3*num_vertices, init_weights=init_exp_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([128, 15069])\ntorch.Size([128, 5023, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "exp_offset = decoder(fc_result)\n",
    "print(exp_offset.shape)\n",
    "exp_offset = torch.reshape(exp_offset, (-1, num_vertices, 3, 1))\n",
    "print(exp_offset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1024, 15069])\ntorch.Size([1024, 5023, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "val_exp_offset = decoder(val_fc_result)\n",
    "print(val_exp_offset.shape)\n",
    "val_exp_offset = torch.reshape(val_exp_offset, (-1, num_vertices, 3, 1))\n",
    "print(val_exp_offset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([128, 5023, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "predicted = exp_offset + face_templates\n",
    "print(predicted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1024, 5023, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "val_predicted = val_exp_offset + val_face_templates\n",
    "print(val_predicted.shape)"
   ]
  }
 ]
}